{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1950e665",
   "metadata": {},
   "source": [
    "# üõë Stop Module Training on Google Colab\n",
    "\n",
    "This notebook trains a **Standalone Stop Detector** using supervised learning to detect DSA curve endpoints.\n",
    "\n",
    "## Features\n",
    "- **Vessel Realism**: Trains with tapering (wide‚Üínarrow) and fading (bright‚Üídim) to match real vessel characteristics\n",
    "- **Balanced Dataset**: Generates equal positive (endpoint) and negative (midpoint) samples\n",
    "- **GPU Accelerated**: Uses Colab's free GPU for fast training\n",
    "- **Validation Metrics**: Tracks per-class accuracy (Stop vs Go)\n",
    "\n",
    "## What You'll Get\n",
    "- A trained stop detector model (`stop_detector_v1.pth`)\n",
    "- Training/validation accuracy metrics\n",
    "- Visualizations of sample crops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and setup\n",
    "!git clone https://github.com/mahsaabadian/DSA-RL-Tracker.git\n",
    "%cd DSA-RL-Tracker\n",
    "!pip install -r Experiment1/requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a3557f",
   "metadata": {},
   "source": [
    "## Check GPU Availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa08665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA / GPU availability\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU Memory:\", f\"{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  GPU not available ‚Äî training will fall back to CPU (slower)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c48b14",
   "metadata": {},
   "source": [
    "## Configure Training Parameters\n",
    "\n",
    "Adjust these parameters as needed:\n",
    "- `epochs`: Number of training epochs (default: 15)\n",
    "- `batch_size`: Batch size (default: 64, increase if GPU memory allows)\n",
    "- `samples`: Samples per class (default: 5000 = 10k total samples)\n",
    "- `learning_rate`: Learning rate (default: 1e-4)\n",
    "- `vessel_realism`: Enable vessel-realistic features (tapering & fading) - **Recommended: True**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07307a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "EPOCHS = 15              # Number of training epochs\n",
    "BATCH_SIZE = 64          # Batch size (increase to 128 if GPU memory allows)\n",
    "SAMPLES_PER_CLASS = 5000 # Samples per class (5000 endpoints + 5000 midpoints = 10k total)\n",
    "LEARNING_RATE = 1e-4     # Learning rate\n",
    "VESSEL_REALISM = True    # Enable vessel-realistic features (tapering & fading)\n",
    "OUTPUT_NAME = \"stop_detector_v1\"  # Output filename (will save as .pth)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Samples per Class: {SAMPLES_PER_CLASS} (Total: {SAMPLES_PER_CLASS * 2})\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Vessel Realism: {VESSEL_REALISM} {'(tapering & fading enabled)' if VESSEL_REALISM else '(uniform curves)'}\")\n",
    "print(f\"  Output: StopModule/weights/{OUTPUT_NAME}.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa037f",
   "metadata": {},
   "source": [
    "## Visualize Sample Data (Optional)\n",
    "\n",
    "Before training, let's visualize what the dataset looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01074e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization of sample crops\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root to path\n",
    "project_root = \"/content/DSA-RL-Tracker\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from Experiment5_reward_config.src.train import CurveMakerFlexible, crop32, load_curve_config\n",
    "\n",
    "# Create a sample curve with vessel realism\n",
    "cfg = load_curve_config()[0]\n",
    "maker = CurveMakerFlexible(h=128, w=128, config=cfg)\n",
    "\n",
    "# Generate sample with vessel realism\n",
    "w_range = (2, 5)\n",
    "img, mask, pts_all = maker.sample_curve(\n",
    "    width_range=w_range,\n",
    "    curvature_factor=1.0,\n",
    "    noise_prob=0.3,\n",
    "    invert_prob=0.5,\n",
    "    width_variation=\"wide_to_narrow\",\n",
    "    start_width=6,\n",
    "    end_width=1,\n",
    "    intensity_variation=\"bright_to_dim\",\n",
    "    start_intensity=0.9,\n",
    "    end_intensity=0.3\n",
    ")\n",
    "\n",
    "pts = pts_all[0]\n",
    "\n",
    "# Show endpoint crop (positive sample)\n",
    "end_pt = pts[-1]\n",
    "path_mask = np.zeros_like(img)\n",
    "for p in pts:\n",
    "    path_mask[int(p[0]), int(p[1])] = 1.0\n",
    "\n",
    "crop_img = crop32(img, int(end_pt[0]), int(end_pt[1]))\n",
    "crop_path = crop32(path_mask, int(end_pt[0]), int(end_pt[1]))\n",
    "\n",
    "# Show midpoint crop (negative sample)\n",
    "mid_idx = len(pts) // 2\n",
    "mid_pt = pts[mid_idx]\n",
    "path_mask_mid = np.zeros_like(img)\n",
    "for p in pts[:mid_idx+1]:\n",
    "    path_mask_mid[int(p[0]), int(p[1])] = 1.0\n",
    "\n",
    "crop_img_mid = crop32(img, int(mid_pt[0]), int(mid_pt[1]))\n",
    "crop_path_mid = crop32(path_mask_mid, int(mid_pt[0]), int(mid_pt[1]))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "# Full curve\n",
    "axes[0, 0].imshow(img, cmap='gray')\n",
    "axes[0, 0].plot(pts[:, 1], pts[:, 0], 'r-', linewidth=2, alpha=0.5)\n",
    "axes[0, 0].plot(end_pt[1], end_pt[0], 'go', markersize=10, label='Endpoint')\n",
    "axes[0, 0].plot(mid_pt[1], mid_pt[0], 'bo', markersize=10, label='Midpoint')\n",
    "axes[0, 0].set_title('Full Curve (Vessel with Tapering & Fading)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Endpoint crop (positive)\n",
    "axes[0, 1].imshow(crop_img, cmap='gray')\n",
    "axes[0, 1].set_title('Endpoint Crop (STOP - Positive)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(crop_path, cmap='jet')\n",
    "axes[0, 2].set_title('Path Mask at Endpoint')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Midpoint crop (negative)\n",
    "axes[1, 0].imshow(img, cmap='gray')\n",
    "axes[1, 0].plot(pts[:, 1], pts[:, 0], 'r-', linewidth=2, alpha=0.5)\n",
    "axes[1, 0].plot(mid_pt[1], mid_pt[0], 'bo', markersize=10)\n",
    "axes[1, 0].set_title('Full Curve (Midpoint Highlighted)')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(crop_img_mid, cmap='gray')\n",
    "axes[1, 1].set_title('Midpoint Crop (GO - Negative)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(crop_path_mid, cmap='jet')\n",
    "axes[1, 2].set_title('Path Mask at Midpoint')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sample visualization complete!\")\n",
    "print(\"   Top row: Endpoint (STOP) - narrow and faded\")\n",
    "print(\"   Bottom row: Midpoint (GO) - wider and brighter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f36d4",
   "metadata": {},
   "source": [
    "## Train Stop Detector\n",
    "\n",
    "This will:\n",
    "1. Generate balanced dataset (endpoints vs midpoints)\n",
    "2. Split into train/validation (80/20)\n",
    "3. Train CNN classifier\n",
    "4. Save best model based on validation accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c34128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "%cd /content/DSA-RL-Tracker/StopModule\n",
    "\n",
    "# Build command\n",
    "cmd = f\"python -u src/train_standalone.py --epochs {EPOCHS} --batch_size {BATCH_SIZE} --samples {SAMPLES_PER_CLASS} --lr {LEARNING_RATE} --output weights/{OUTPUT_NAME}.pth\"\n",
    "\n",
    "if not VESSEL_REALISM:\n",
    "    cmd += \" --no_vessel_realism\"\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"Command: {cmd}\\n\")\n",
    "!{cmd}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e08401",
   "metadata": {},
   "source": [
    "## Training Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if weights were saved\n",
    "import os\n",
    "weight_path = f\"/content/DSA-RL-Tracker/StopModule/weights/{OUTPUT_NAME}.pth\"\n",
    "\n",
    "if os.path.exists(weight_path):\n",
    "    file_size = os.path.getsize(weight_path) / 1024  # KB\n",
    "    print(f\"‚úÖ Model saved successfully!\")\n",
    "    print(f\"   Path: {weight_path}\")\n",
    "    print(f\"   Size: {file_size:.1f} KB\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Weight file not found at: {weight_path}\")\n",
    "    print(\"   Checking for alternative locations...\")\n",
    "    weights_dir = \"/content/DSA-RL-Tracker/StopModule/weights\"\n",
    "    if os.path.exists(weights_dir):\n",
    "        files = os.listdir(weights_dir)\n",
    "        if files:\n",
    "            print(f\"   Found files: {files}\")\n",
    "        else:\n",
    "            print(\"   No weight files found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f574a",
   "metadata": {},
   "source": [
    "## Test the Trained Model (Optional)\n",
    "\n",
    "Load and test the model on some sample curves:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of the trained model\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root\n",
    "project_root = \"/content/DSA-RL-Tracker\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from StopModule.src.models import StandaloneStopDetector\n",
    "from Experiment5_reward_config.src.train import CurveMakerFlexible, crop32, load_curve_config\n",
    "\n",
    "# Load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = StandaloneStopDetector().to(device)\n",
    "weight_path = f\"/content/DSA-RL-Tracker/StopModule/weights/{OUTPUT_NAME}.pth\"\n",
    "\n",
    "if os.path.exists(weight_path):\n",
    "    model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"‚úÖ Model loaded from {weight_path}\")\n",
    "    \n",
    "    # Generate test samples\n",
    "    cfg = load_curve_config()[0]\n",
    "    maker = CurveMakerFlexible(h=128, w=128, config=cfg)\n",
    "    \n",
    "    # Test on endpoint (should predict STOP)\n",
    "    img, mask, pts_all = maker.sample_curve(\n",
    "        width_range=(2, 5),\n",
    "        width_variation=\"wide_to_narrow\",\n",
    "        start_width=6,\n",
    "        end_width=1,\n",
    "        intensity_variation=\"bright_to_dim\",\n",
    "        start_intensity=0.9,\n",
    "        end_intensity=0.3,\n",
    "        noise_prob=0.3\n",
    "    )\n",
    "    pts = pts_all[0]\n",
    "    end_pt = pts[-1]\n",
    "    path_mask = np.zeros_like(img)\n",
    "    for p in pts:\n",
    "        path_mask[int(p[0]), int(p[1])] = 1.0\n",
    "    \n",
    "    crop_img = crop32(img, int(end_pt[0]), int(end_pt[1]))\n",
    "    crop_path = crop32(path_mask, int(end_pt[0]), int(end_pt[1]))\n",
    "    crop_input = torch.tensor(np.stack([crop_img, crop_path], axis=0), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logit = model(crop_input)\n",
    "        prob = torch.sigmoid(logit).item()\n",
    "    \n",
    "    print(f\"\\nüìä Test Results:\")\n",
    "    print(f\"   Endpoint prediction: {prob:.3f} ({'STOP' if prob > 0.5 else 'GO'})\")\n",
    "    print(f\"   Expected: STOP (probability should be > 0.5)\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axes[0].imshow(img, cmap='gray')\n",
    "    axes[0].plot(pts[:, 1], pts[:, 0], 'r-', linewidth=2, alpha=0.5)\n",
    "    axes[0].plot(end_pt[1], end_pt[0], 'go', markersize=10)\n",
    "    axes[0].set_title(f'Full Curve\\nPrediction: {\"STOP\" if prob > 0.5 else \"GO\"} ({prob:.3f})')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(crop_img, cmap='gray')\n",
    "    axes[1].set_title('Image Crop')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(crop_path, cmap='jet')\n",
    "    axes[2].set_title('Path Mask')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Model file not found: {weight_path}\")\n",
    "    print(\"   Please run training first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3d2cc",
   "metadata": {},
   "source": [
    "## Download Trained Weights\n",
    "\n",
    "Download the trained model weights to your local machine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download weights\n",
    "from google.colab import files\n",
    "\n",
    "weight_path = f\"/content/DSA-RL-Tracker/StopModule/weights/{OUTPUT_NAME}.pth\"\n",
    "\n",
    "if os.path.exists(weight_path):\n",
    "    print(f\"üì• Downloading {OUTPUT_NAME}.pth...\")\n",
    "    files.download(weight_path)\n",
    "    print(\"‚úÖ Download complete!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  File not found: {weight_path}\")\n",
    "    print(\"   Please check the training output above for the correct path\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066fe562",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After training, you can:\n",
    "\n",
    "1. **Use the model in FineTune module**: Load these weights to replace RL-trained stop heads\n",
    "2. **Integrate into RL training**: Use as a pretrained stop detector\n",
    "3. **Test on real DSA images**: Use the model for inference on actual vessel images\n",
    "\n",
    "The model file (`stop_detector_v1.pth`) contains the trained weights and can be loaded with:\n",
    "```python\n",
    "from StopModule.src.models import StandaloneStopDetector\n",
    "model = StandaloneStopDetector()\n",
    "model.load_state_dict(torch.load('stop_detector_v1.pth'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc56ae0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
